{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning入門\n",
    "\n",
    "## テストデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import model_to_dot, to_categorical\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "fig = plt.figure(figsize=(9, 15))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0,\n",
    "                    top=0.5, hspace=0.05, wspace=0.05)\n",
    "\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(1, 9, i + 1, xticks=[], yticks=[])\n",
    "    ax.set_title(str(y_train[i]))\n",
    "    ax.imshow(x_train[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 入力画像を行列(28x28)からベクトル(長さ784)に変換\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "# 名義尺度の値をone-hot表現へ変換\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 最初のlayerはinput_shapeを指定して、入力するデータの次元を与える必要がある\n",
    "# Dense: 一般的な全結合層を表すレイヤー\n",
    "# 初期化: Heの初期化法は活性化関数がReLUであるときに適している\n",
    "model.add(Dense(units=256, input_shape=(784,),\n",
    "                kernel_initializer='he_uniform'))\n",
    "# Activation: 活性化関数として relu を選択\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 同時に指定も可能\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# ドロップアウト: 近似的にアンサンブル法を実現するもの\n",
    "# ドロップアウトは入力の一部をランダムに0にして出力するlayerの一種。\n",
    "# 訓練データセットから部分訓練データセットを大量に作成し、\n",
    "# 各モデルの予測結果を平均する手法をアンサンブルというが、\n",
    "# とてつもない計算量を要する\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# 正規化: L2正則化では、全パラメータの2乗和を正則化項として損失関数に加えます。\n",
    "# L2正則化では、パラメータを完全に0にすることは少ないものの、\n",
    "# パラメータを滑らかにすることで予測精度のより良いモデルを構築する\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "# 正規化: L1正則化では、全パラメータの絶対値の和を正則化項として損失関数に加える。\n",
    "# L1正則化ではL2正則化よりもパラメータが0になりやすいという特徴（スパース性）がある\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l1(0.01)))\n",
    "\n",
    "# 正規化: L1正則化とL2正則化の組み合わせのElasticNet\n",
    "model.add(Dense(100, activation='relu',\n",
    "                kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)))\n",
    "\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    # optimizer='sgd',\n",
    "    optimizer=Adam(),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, dpi=72).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=1000, epochs=20, verbose=1,\n",
    "    validation_data=(x_test, y_test),\n",
    "    # 早期終了: 検証データの誤差が大きくなってきた（或いは評価関数値が下がってきた）ところで学習をストップさせる\n",
    "    callbacks=[EarlyStopping(patience=0, verbose=1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 活性化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, sigmoid(x), label='sigmoid')\n",
    "ax.plot(x, relu(x), label='ReLU')\n",
    "ax.plot(x, tanh(x), label='tanh')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-1.1, 2)\n",
    "plt.grid(which='major', color='gray', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 活性化関数の微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def deriv_sigmoid(x):\n",
    "    return np.exp(x) / (1 + np.exp(x))**2\n",
    "\n",
    "\n",
    "def deriv_tanh(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "\n",
    "def deriv_relu(x):\n",
    "    return 1 * (x > 0)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "# sigmoid\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, deriv_sigmoid(x), label='sigmoid deriv')\n",
    "\n",
    "# tanh\n",
    "ax.plot(x, deriv_tanh(x), label='tanh deriv')\n",
    "\n",
    "# relu\n",
    "ax.plot(x, deriv_relu(x), label='ReLU deriv')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(0, 1.2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
